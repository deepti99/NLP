# -*- coding: utf-8 -*-
"""NLP Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v34ahKR5NacCuhxAq2GciajDvOXHmLM9
"""

pip install nltk

import nltk

nltk.download('all')

nltk.download('stopwords')
import nltk
#In NLTK, PUNKT is an unsupervised trainable model, which means it can
#be trained on unlabeled data (Data that has not been tagged with
#information identifying its characteristics, properties, or categories is referred
#to as unlabeled data.)
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag

sentence = "I am Deepti. I love shopping and playing games!."+

"""**Step 1 : Stop words removal**"""

mystopwords = set(stopwords.words('english')+['``',"''",'.',"!"])
tokenizedsentence = word_tokenize(sentence)

finalsentence = [w for w in tokenizedsentence if not w in mystopwords]
print (finalsentence)

"""**Step 2 : Stemming**"""

import nltk
from nltk.stem import PorterStemmer
ps = PorterStemmer()

sentence = "Natural Language Processing is great. I love this subject."

split = print(sentence.split())

for word in sentence.split():
  print (ps.stem(word))

"""**Step 3: Lemmatization**"""

# import these modules
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

print("rocks :", lemmatizer.lemmatize("rocks"))
print("corpora :", lemmatizer.lemmatize("corpora"))
print("rocks :", lemmatizer.lemmatize("rocks"))
print("books :", lemmatizer.lemmatize("books"))
print("kites:", lemmatizer.lemmatize("kites"))
print("mangoes:", lemmatizer.lemmatize("mangoes"))
print("knives:", lemmatizer.lemmatize("knives"))
print("wives:", lemmatizer.lemmatize("wives"))
print("wolves:", lemmatizer.lemmatize("wolves"))

import nltk

# download required nltk packages
# required for tokenization
nltk.download('punkt')
# required for parts of speech tagging
nltk.download('averaged_perceptron_tagger')

# input text
sentence = """Today morning, Arthur felt very good."""

# tokene into words
tokens = nltk.word_tokenize(sentence)

# parts of speech tagging
tagged = nltk.pos_tag(tokens)

# print tagged tokens
print(tagged)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
stop_words = set(stopwords.words('english'))


txt = "Sukanya, Rajib and Naba are my good friends. " \
	"Sukanya is getting married next year. " \
	"Marriage is a big step in oneâ€™s life." \
	"It is both exciting and frightening. " \
	"But friendship is a sacred bond between people." \
	"It is a special kind of love between us. " \
	"Many of you must have tried searching for a friend "\
	"but never found the right one."

# sent_tokenize is one of instances of
# PunktSentenceTokenizer from the nltk.tokenize.punkt module

tokenized = sent_tokenize(txt)
for i in tokenized:

	# Word tokenizers is used to find the words
	# and punctuation in a string
	wordsList = nltk.word_tokenize(i)

	# removing stop words from wordList
	wordsList = [w for w in wordsList if not w in stop_words]

	# Using a Tagger. Which is part-of-speech
	# tagger or POS-tagger.
	tagged = nltk.pos_tag(wordsList)

	print(tagged)